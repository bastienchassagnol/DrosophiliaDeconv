[{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/CONTRIBUTING.html","id":null,"dir":"","previous_headings":"","what":"Contributing to DrosophiliaDeconv","title":"Contributing to DrosophiliaDeconv","text":"outlines propose change DrosophiliaDeconv. detailed discussion contributing tidyverse packages, please see development contributing guide code review principles.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/CONTRIBUTING.html","id":"fixing-typos","dir":"","previous_headings":"","what":"Fixing typos","title":"Contributing to DrosophiliaDeconv","text":"can fix typos, spelling mistakes, grammatical errors documentation directly using GitHub web interface, long changes made source file. generally means ’ll need edit roxygen2 comments .R, .Rd file. can find .R file generates .Rd reading comment first line.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/CONTRIBUTING.html","id":"bigger-changes","dir":"","previous_headings":"","what":"Bigger changes","title":"Contributing to DrosophiliaDeconv","text":"want make bigger change, ’s good idea first file issue make sure someone team agrees ’s needed. ’ve found bug, please file issue illustrates bug minimal reprex (also help write unit test, needed). See guide create great issue advice.","code":""},{"path":[]},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/CONTRIBUTING.html","id":"reporting-issues","dir":"","previous_headings":"Pull request process","what":"Reporting Issues","title":"Contributing to DrosophiliaDeconv","text":"encounter bug, question, want request new feature: - Search issues see already reported discussed. - , create new issue. Please include: - clear descriptive title. - detailed description issue request. - Steps reproduce bug (applicable). - Screenshots logs (available). Fork package clone onto computer. haven’t done , recommend using usethis::create_from_github(\"bastienchassagnol/DrosophiliaDeconv\", fork = TRUE). Install development dependencies devtools::install_dev_deps(), make sure package passes R CMD check running devtools::check(). R CMD check doesn’t pass cleanly, ’s good idea ask help continuing. Create Git branch pull request (PR). recommend using usethis::pr_init(\"brief-description--change\"). Make changes, commit git, create PR running usethis::pr_push(), following prompts browser. title PR briefly describe change. body PR contain Fixes #issue-number.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/CONTRIBUTING.html","id":"code-style","dir":"","previous_headings":"Pull request process","what":"Code style","title":"Contributing to DrosophiliaDeconv","text":"New code follow tidyverse style guide. can use styler package apply styles, please don’t restyle code nothing PR. use roxygen2, Markdown syntax, documentation. use testthat unit tests. Contributions test cases included easier accept.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/CONTRIBUTING.html","id":"publication-to-bioinformatics-advances","dir":"","previous_headings":"Pull request process","what":"Publication to Bioinformatics advances","title":"Contributing to DrosophiliaDeconv","text":"Read first General instructions authors, followed Instructions Authors. Submission form General template stored article folder","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2024 DrosophiliaDeconv authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"protocol-to-download-and-retrieve-metadata-to-build-reference-datasets","dir":"Articles","previous_headings":"","what":"Protocol to download and retrieve metadata to build reference datasets","title":"Explore and download sequencing databases: hows-to","text":"nice HTML + quarto presentation, functional figure, lst float references; citations nice call-boxes, download locally self-embedded HTML file download.html.","code":"# Tidyverse libraries library(knitr) library(ggplot2) library(dplyr) library(stringr) # For regex operations library(purrr) # vectorial operations library(readr) # read and write tsv and csv files  #  For website exploration library(rvest)  # For web scraping and HTML parsing library(jsonlite) library(httr2) # tailored for HTTP/HTTPS requests # both packages are better tailored to FTP protocols library(curl)  library(RCurl)  # For SQL parsing and integration with R library(DBI) # open connection with DBMS library(dbplyr) # seamless use of dplyr operations with SQL power library(RSQLite) # lightweight DBMS   # Set global options opts_chunk$set(   echo = TRUE,         # Show R code in the output   eval = FALSE,        # By default, do not execute R code   warning = FALSE,     # Suppress warnings in the output   message = FALSE,     # Suppress messages in the output   fig.width = 7,       # Default figure width   fig.height = 5,      # Default figure height   fig.align = \"center\" # Centre-align figures )  # Set ggplot2 minimal theme for consistency theme_set(theme_minimal())"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"litterature-and-web-scraping","dir":"Articles","previous_headings":"","what":"Litterature and web-scraping","title":"Explore and download sequencing databases: hows-to","text":"FlyBase:Drosophila_Online_Resources Wiki fly page also refers list tools, Web APIs curated pipelines analyse Drosophila samples. popular relevant studies modENCODE tissue cell lines experiences FlyAtlas21. List scRNASeq datasets tools, tissue level Comprehensive list scRNASeq databases, within tissue level, using DRscDB Shiny App Complete list Datasets, FlyAtlas compendium datasets, metadata characterising , reported Excel table data/databases_metadata.xlsx, composed three sheets: Databases references, enumerating relevant datasets RNASeq resources. DRscDB Databases Single Cell, listing scRNASeq datasets higher granularity tissue level (instance, characterising cell subtypes composing digestive tract).","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"optionnal-retrieve-programmatically-drscdb-single-cell-databases","dir":"Articles","previous_headings":"Litterature and web-scraping","what":"(Optionnal) Retrieve programmatically DRscDB single cell databases","title":"Explore and download sequencing databases: hows-to","text":"Toggle DRscDB Datasets tab. Retrieve metadata dataset using R web-scraping tool rvest, select relevant columns:","code":"url_DRscDB <- \"https://www.flyrnai.org/tools/single_cell/web/summary\" irrelevant_columns_DRscDB <- c(\"pubmed date\", \"dataset ids\", \"subset names\",                                \"species\", \"species id\",                                \"reads cell\", \"genes cell\", \"total gene\",                                \"batch correction\", \"data in supp table\",                                \"web link available\", \"linkout URL\") metadata_DRscDB <- read_html(url_DRscDB) |> # read a HTML file   html_table() |> # from the HTML file, select only Tables (on that website, only one found)   purrr::pluck(1) |>   filter(species %in% \"Drosophila\") |> # Filter on Drosophila species   select(-all_of(irrelevant_columns_DRscDB))"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"retrieve-phenotype-annotation-for-each-dataset","dir":"Articles","previous_headings":"","what":"Retrieve phenotype annotation for each dataset","title":"Explore and download sequencing databases: hows-to","text":"homogeneous database storage (SQL, Excel, SRA, tsv file, …), within -studies FlyBase database comprehensive, store properly annotation data tissue, least RNASeq samples2. official --date BioConductor CRAN package fetch explore Drosophila databases Consequence: specific protocol retrieving phenotypical annotation dataset (enumerate top three popular ones).","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"sec-modencode-metadata","dir":"Articles","previous_headings":"Retrieve phenotype annotation for each dataset","what":"The ModENCODE pheno data","title":"Explore and download sequencing databases: hows-to","text":"easiest way find: retrieve metadata JBrowse FlyBase Wiki website pasting HTML table Excel Workbook, use Paste refreshable web query option FlyBase RNASeq Search profile, clicking tissue optionally cell line items. finalise annotation, ’re going use Rest API GET HitList, programmatic access enabled FlyBase website:","code":"ModENCODE_databases <- read_html(\"https://wiki.flybase.org/wiki/FlyBase:ModENCODE_data_at_FlyBase#RNA-Seq_JBrowse_Track_Listing\") |>    html_table() |> # `trim = FALSE` to ensure missing cells are filled correctly    purrr::pluck(1) |>    filter(`Track section` %in% c(\"Expression>RNA-Seq >modENCODE Transcriptomes  >Tissues\"))   ModENCODE_databases_formatted <- ModENCODE_databases |>    dplyr::filter(stringr::str_detect(`Track section`, \"Tissues$\")) |>    select(-`Track section`, - \"JBrowse Track Description\") |>    rename(tissue = \"Track name\", FlybaseID=\"FlyBase Dataset(s)\") |>    mutate(FlybaseID = lapply(stringr::str_split(FlybaseID,        # split by mE_mRNA, without consuming the separator                    pattern = \"(?=mE_mRNA)\"), str_subset, \"[^ ]\")) |>  # remove empty chains    tidyr::unchop(FlybaseID) # from compact list of samples per tissue type to tidy format  readr::write_csv(ModENCODE_databases_formatted,                   \"./data/RNASeq/ModENCODE_pheno_data.csv\") # Which individual experiences we want metadata from? flybase_ids <- c(\"FBlc0000215\", \"FBlc0000216\", \"FBlc0000217\")  # Create a URL with IDs delimited by commas base_url <- \"https://api.flybase.org/api/v1.0/hitlist/fetch\" modENCODE_request <- request(base_url) |>   # parameter construction defined here: https://flybase.github.io/api/swagger-ui/#/HitList/getFlyBaseHitList   req_url_path_append(flybase_ids |>                          paste(collapse = \",\")) |>    req_perform()  # Check the response status, 200 corresponding to a success if (resp_status(modENCODE_request) == 200 & resp_has_body(modENCODE_request)) {   # Parse the JSON content into an R object   ModENCODE_metadata <-   modENCODE_request |>      resp_body_string() |>      fromJSON()  |>      purrr::pluck(\"resultset\") |>      purrr::pluck(\"result\") } else {   # convert a HTTP error to a R error   resp_check_status(modENCODE_request) }"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"the-flyatlas2-pheno-data","dir":"Articles","previous_headings":"Retrieve phenotype annotation for each dataset","what":"The FlyAtlas2 pheno data","title":"Explore and download sequencing databases: hows-to","text":"Excel Workbook mentioned Docs tab section official FlyAtlas2 initiative stores aggregated averaged information tissue level, rendering useless proper downstream analyses3. conclusion, method dataset irrelevant downstream analyses4 FPKM/RPKM file stored web encyclopaedia FlyBase, column headers identifying uniquely sample describe developmental stage, sex original tissue. However, ’s really practical parse, follows unique identifier composition. Example: RNA-Seq_Profile_FlyAtlas2_Adult_Female_Brain_(FBlc0003619). However, can use REST API trick mentioned (lst-API-FlyBase-modENCODE-metadata?) get additional information (column title). challenging method involves re-running SQL file provided project. name SQL file can retrieved FlyAtlas2 website, , somehow twisted way following R code5. previous manipulation, two files available: motif.mvls.gla.ac.uk/downloads/FlyAtlas2_2024.01.08.sql, motif.mvls.gla.ac.uk/downloads/FlyAtlas2_gene_data.xlsx, SQL file comprehensive, storing gene expressions individual level (aggregated tissue level). . SQL relational database described details Supplementary FlyAtlas 2 documents, Table S2, Pages 3-4. interest, Tissue, GeneFPKM GeneRPM datasets6. ii. Parse SQL file’s instructions, keeping SQL lines building Tissue, GeneFPKM GeneRPM datasets. Usually, building SQL dataset involves three stages: ensuring non prior existence dataset (’s case, enforce deletion), define dataset (name type features/columns), finally, build dataset (indeed, dataset content hard-coded within SQL file): Use modern SQL DBMS. Report section (sec-convert-sql-format?) details. Major issue current Fly Atlas 2 database outdated use myISAM DBMS standards, official converter exists. (Optional) Verify updated SQL instructions can parsed executed using local DBMS manager like mySQL, even simpler, going SQL online resources. turned remove uniqueness constraint Tissue.Abbreviation feature run SQL code. Execute modern SQL file, store transiently output SQLite database, collect export results standard csv file.","code":"flyatlas_url <- \"https://flyatlas.gla.ac.uk/FlyAtlas2/index.html?page=help\" webpage_text <- read_html(flyatlas_url) |>    html_text() flyatlas_files <- stringr::str_extract_all(webpage_text,                                           \"motif.mvls.gla.ac.uk/downloads/[^\\\\s]+\", simplify = TRUE) |> as.vector() sql_instructions <- readr::read_lines('data/FlyAtlas2_2024.01.08.sql',                                       skip_empty_rows = TRUE) |>   # Keep only lines that are NOT empty nor commented (defined by \"--\")   # stringr::str_subset(\"^\\\\s*(--)|/\\\\*\", negate = TRUE) |>   stringr::str_subset(\"^\\\\s*(--)|/\\\\*|(UN)?LOCK\", negate = TRUE) |>   paste(collapse = \"\\r\\n\") |>   str_extract_all(pattern = regex(\"[^;]+;\", multiline = TRUE,dotall = TRUE),                   simplify = TRUE) |>   as.vector()  tissue_index <- grep(\"CREATE TABLE `Tissue`\", sql_instructions, fixed = TRUE) tissue_instructions <- sql_instructions[(tissue_index-1):(tissue_index + 1)] readr::write_lines(tissue_instructions,                    \"data/old_tissue_parsing.sql\") # create SQLite database connection con <- dbConnect(RSQLite::SQLite(), \"data/FlyAtlas2_tissue.sqlite\")  sql_statements <- readr::read_lines(\"data/new_tissue.sql\",                                      skip_empty_rows = TRUE) |>    stringr::str_subset(\"^\\\\s*(--)|/\\\\*|(UN)?LOCK\", negate = TRUE) |>   paste(collapse = \"\\r\\n\") |>   str_extract_all(pattern = regex(\"[^;]+;\", multiline = TRUE,dotall = TRUE),                   simplify = TRUE) |>    as.vector()  # Tip: since dbExecute can only perform one query after the other, we use a for-loop to perform sequential SQL queries for (statement in sql_statements) {     dbExecute(con, statement) }  # testthat::expect_contains(dbListTables(con), \"Tissue\") # DBI::dbRemoveTable(con, \"Tissue\")  # Export the tissue dataset to a standard CSV format tissue_fly_atlas <- tbl(con, \"Tissue\") |>    collect() readr::write_csv(tissue_fly_atlas,                  \"data/pheno_data_flyatlas2.csv\") # Close the connection once done dbDisconnect(con)"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"sec-convert-sql-format","dir":"Articles","previous_headings":"Retrieve phenotype annotation for each dataset","what":"(Optionnal) Convert old SQL version 5 to modern SQL version 9","title":"Explore and download sequencing databases: hows-to","text":"functionalities going along DBI R package theoretically enable us execute SQL instructions directly R, turns SQL instructions tailored MyISAM DBMS software, outdated 2009, can processed directly DBI wrapper. Besides, DBI works best SQLite DBMS, FlyBase SQL file written end. conclude, convert SQL FlyBase file modern SQL standards (current version: SQL Server 2022) ensuring compatibility SQLite. dedicated tools, knowledge, developed R end. definitely, best method, intrinsically stochastic, consist pairing LLM using API REST integration prompt engineering. propose two code snippets personalised engineered prompt, using chattr, see (lst-chattr-sql?) tidychatmodels, see (lst-tidychatmodels-sql?), respectively7. finally, alternative SQL method, mixes bunch regex expressions tidyverse manipulations, guaranteeing determined output:","code":"# remotes::install_github(\"mlverse/chattr\") library(chattr)  # parameter configuration and saving chattr_use(\"copilot\") chattr_defaults(max_data_files = 10,                  include_doc_contents = TRUE,                  max_data_frames = 10,                 yaml_file = \"vignettes/chattr.yml\")  new_sql <- chattr(prompt = \"Please convert the SQL file <old-sql-location>, written with SQL version 5, to SQL Server 2022 syntax, while making it compliant with the SQLite DBMS constraints.\",        prompt_build = \"Specifically, focus on the following: Change data types to match SQLite standards (e.g., TINYINT to INTEGER, ENUM to TEXT). Replace AUTO_INCREMENT with AUTOINCREMENT. Remove any unsupported SQL syntax in SQLite. Write the resulting modernised SQL file to `data/new_tissue_global.sql`\") # devtools::install_github(\"AlbertRapp/tidychatmodels\") library(tidychatmodels)  chat_openai <- create_chat('openai', Sys.getenv('OAI_DEV_KEY'))|>   add_model('gpt-3.5-turbo') |>   # add_params(temperature = 0.5, max_tokens = 100) |>   # define the general AI agent system   add_message(     role = 'system',     message = 'You are a chatbot that only returns SQL code, whose syntax is     compatible with SQL Server 2022 and SQLite DBMS.'   ) |>   # create your customised prompt   add_message(     role = 'user',     message = 'Convert old sql file <old-sql-path> to recent SQL code,     write the output to <new-sql-path>. Specifically: Update data types to align     with SQLite standards, such as replacing TINYINT with INTEGER,     ENUM with TEXT, and other incompatible types. Change AUTO_INCREMENT to     AUTOINCREMENT for primary keys. Remove syntax elements unsupported in SQLite,     like full-text indexes, foreign key constraints outside PRAGMA, or stored procedures.     Where conflicts between SQL Server and SQLite arise, prioritize SQLite compliance.     Output the transformed SQL file as a set of discrete statements, one per line,     removing empty or commented lines for clarity. Do not add any empty lines nor comments. Do not add uniqueness constraints. Convert the entire SQL file, without using further rows argument.'   )  chat_user_result <- chat_openai |>   perform_chat()|>   extract_chat(silent = TRUE) tissue_colnames <- grep(\"CREATE TABLE `Tissue`\", sql_instructions,                         fixed = TRUE, value = TRUE) |>    stringr::str_extract_all(pattern = \"(?<=\\\\`)[[:alnum:]]+(?=\\\\`)\",                            simplify = TRUE) |>   as.vector() |>    unique() |>    setdiff(\"Tissue\")  tissue_inputs <- grep(\"INSERT INTO `Tissue`\", sql_instructions,                       fixed = TRUE, value = TRUE) |>   stringr::str_extract_all(\"(?<=\\\\()([^)]+)(?=\\\\),*)\",                                                  simplify = TRUE) |>   as.vector() |>   stringr::str_replace_all(\"'\", \"\") |>   purrr::map(\\(x) stringr::str_split(x, \",\", simplify = TRUE) |>                tibble::as_tibble()) |>   purrr::list_rbind() |>   setNames(nm = tissue_colnames)  readr::write_csv(tissue_inputs,                  file = \"data/pheno_data_flyatlas2.csv\",                  escape = \"none\", quote = \"none\")"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"sec-fca-pheno-data","dir":"Articles","previous_headings":"Retrieve phenotype annotation for each dataset","what":"The single cell Fly Cell Atlas pheno data","title":"Explore and download sequencing databases: hows-to","text":"One possibility retrieve phenotype data consist using BioConductor ArrayExpress package. Unfortunately, seem updated anymore, indeed unable download parse two FCA experiences (actually, even examples package documentation can’t executed anymore)8. Typical pipeline illustrated : Alternatively, can download 10X Smartseq2 experiences expression data count level (raw applying TPM normalisation) using following Single cell Expression Atlas web query, crossing datasets, finally clicking Download 2 entries button feature. can perform simultaneously datasets, one dataset , illustrated (lst-array-express-https-protocol?) https protocol (subsetting files available protocol) ftp protocol, (lst-array-express-ftp-protocol?)9: ArrayExpress datasets traditionally organized following key file types, ordered increasing level detail: Investigation Description Format (IDF) provides high-level overview experiment. Title Description: Summary experiment’s purpose. Design Protocols: Experimental design factors studied (e.g., treatments, time points). Contacts: Information authors submitters. Sample Data Relationship Format (SDRF): phenotype data, detailing mapping Sample Identifiers, Factor Values Data File Names Array Design Format (ADF) used provide design microarray platforms used experiment. Equivalent featureData, setting mapping genes sampled modern standard equivalents. Expression matrices, : Raw Data Files (FASTQ (sequencing data) CEL (Affymetrix arrays)) Processed Data Files (usually counts, normalization transformation)","code":"#if (!requireNamespace(\"BiocManager\", quietly = TRUE)) #    install.packages(\"BiocManager\") # BiocManager::install(\"ArrayExpress\") library(ArrayExpress)  # download both processed and raw datasets ae_files <- getAE(\"E-MTAB-10628\", type = \"full\", path = \"data/Fly_Cell_Atlas_10x/\")  ## Build a an ExpressionSet from the raw data ae_data_raw <- ae2bioc(mageFiles = ae_files)  ## Build a an ExpressionSet from the processed data ae_data <- procset(ae_files, getcolproc(ae_files)[2])  # equivalent to run ArrayExpress function ae_data <- ArrayExpress(\"E-MTAB-10628\", path = \"data/Fly_Cell_Atlas_10x/\",                         save = TRUE, dataCols = NULL)   # use methods from the Biobase::ExpressionSet object to retrieve respectively  pheno_data <- phenoData(ae_data) # phenotype data experiment_data <- experimentData(ae_data) # MIAME experiment annotation (metadata) gene_annotation <- fData(ae_data) # gene feature annotation expression_matrix <- exprs(ae_data) # expression matrix data  # worth noted that the ExpressionSet object is well tailored for bulk micro-array  # and RNASeq, but not really for singlecell RNASeq experiences # \"https://www.ebi.ac.uk/gxa/sc/experiment/E-MTAB-10628/download/zip?fileType=experiment-metadata&accessKey=\" # \"https://www.ebi.ac.uk/gxa/sc/experiment/E-MTAB-10628/download?fileType=experiment-design&accessKey=\" #   https://www.ebi.ac.uk/gxa/sc/experiment/E-MTAB-10628/download/zip?fileType=normalised&accessKey= #   https://www.ebi.ac.uk/gxa/sc/experiment/E-MTAB-10628/download/zip?fileType=quantification-raw&accessKey=    base_url <- \"https://www.ebi.ac.uk/gxa/sc/experiment/\" array_identifer <- \"E-MTAB-10628\" # the SmartSeq identifier # array_identifer <- \"E-MTAB-10519\" # the 10X identifier # we took the example of the metadata zip file, containing idf and sdrf files # should be extended to normalised and raw expression files + experiment design metadata_path <- file.path(\"./data/Fly_Cell_Atlas_10x\",                            paste0(array_identifer, \"-experiment-metadata\", \".zip\")) fca_request <- request(base_url) |>   req_url_path_append(array_identifer, \"download\", \"zip\") |>    req_url_query(fileType=\"experiment-metadata\", accessKey=\"\") |>    req_progress(type = \"down\") |>    req_perform(path = metadata_path)   if (resp_has_body(fca_request) & !resp_is_error(fca_request)) {   zip::unzip(metadata_path,               overwrite = TRUE, exdir = \"./data/Fly_Cell_Atlas_10x\")   file.remove(metadata_path) } base_url <- \"ftp.ebi.ac.uk/pub/databases/microarray/data/atlas/sc_experiments\" array_identifier <- \"E-MTAB-10628\" ftp_url <- paste0(base_url, \"/\", array_identifier, \"/\")  # List all files in the FTP directory ftp_filenames <- curl_fetch_memory(ftp_url) |>   purrr::pluck(\"content\") |>   rawToChar() |>   # Split by rows (newline character)   str_split( pattern = \"\\r\\n\", simplify = TRUE) |>   str_subset(\"[^ ]\") |>  # remove empty lines |>   # matches one or more characters that are not whitespace, to the end of the string.   stringr::str_extract( pattern = \"[^\\\\s]+$\")  idf_index <- grep(pattern = \"idf\", ftp_URLs) ftp_URLs <- paste0(ftp_url, ftp_filenames) download.file(ftp_URLs[idf_index],               destfile = ftp_filenames[idf_index],               method = \"auto\")"},{"path":[]},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"sec-modencode-expression","dir":"Articles","previous_headings":"Retrieve transcriptomic annotation","what":"The ModENCODE expression matrix","title":"Explore and download sequencing databases: hows-to","text":"current RPKM database storing bulk RNASeq experiences comprehensive FlyBase data collection can retrieved . Alternatively, can explore FTP repository, sub-setting files’ selection mentioning gene_rpkm explicitly name10 case, 0 store gene expression data, namely : NA: Table description. Note ModENCODE data provided RPKM pre-processed RNA-Seq expression values, FlyAtlas2 data normalised FPKM units. NA: Table description","code":"# FTP URL for the repository ftp_url <- \"ftp://ftp.flybase.net/releases/current/precomputed_files/genes/\" # List all files in the FTP directory ftp_files <- curl_fetch_memory(ftp_url) |>    purrr::pluck(\"content\") |>    rawToChar() |>    # Split by rows (newline character)   str_split( pattern = \"\\r\\n\", simplify = TRUE) |>    str_subset(\"[^ ]\") # remove empty lines  # Split each row by columns (tab character) and convert to a data.frame ftp_files <- as_tibble(do.call(rbind,                                 str_split(ftp_files, pattern = \"[[:blank:]]{2,}\")))  ftp_files <- ftp_files |>    tidyr::separate_wider_delim(cols = \"V4\",                               delim = \" \",                                too_many = c(\"drop\"), # silently remove file renaming                               names = c(\"size\", \"month\", \"day\", \"hour\", \"filename\"))  rnaseq_ftp_files <- stringr::str_subset(ftp_files$filename,                                           pattern = \"gene_rpkm\")"},{"path":[]},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"the-fly-cell-atlas-expression-matrix","dir":"Articles","previous_headings":"Retrieve transcriptomic annotation","what":"The Fly Cell Atlas expression matrix","title":"Explore and download sequencing databases: hows-to","text":"Depending level resolution, new Biostudies website, aiming replacing ArrayExpress web server, provides two APIs: download files expression level (counts), normalised raw counts, already detailed programmatic process (sec-fca-pheno-data?). Note addition protocol detailed previous section, may alternatively use FTP, curl, Aspera Globus software. However, tools usually require know advance file locations paths downloaded. tools detailed Biostudies documention. ENA Portal API (see (lst-ena-ebi-api?) programmatic access) can used retrieve raw files, sequence level (words, bio-informatician job, possibility retrieve FASTQ BAM files, latter corresponding sequences already aligned reference genome). requires retrieve corresponding ENA Project accession number (usually, PRJ... main identifier, ERP... second, optional identifier). E-MTAB-10628 paired PRJEB45993 project (ENA identifier: ERP130174) , E-MTAB-10519 paired PRJEB45570 (ENA identifier: ERP129698). documentation details can found , interactive swagger UI reported . Without mapping information, can browse Biostudies website (report (fig-ERP-MAGE?) snapshot), programmatically either parsing idf file retrieving json file associated Biostudies online accession (see (lst-mtab-ena-mapping?)). note, without REST API programmatic access, BioConductor SRAdb package provides programmatic access metadata Sequence Read Archive (SRA) ENA (European Nucleotide Archive) R11. However, relies strong SQLite dependency, maintained two years, requires prior installation SQL metadata database, implying strong storage memory-resource. Besides, search database streamlined development ENA Browser API. methods non-programmers reported Download Files ENA Documentation, popular tools like ENA Browser, ENA FTP Downloader GUI tool, Globus Aspera. practical downloading thousands files, dedicated facilities manage parallel downloading error handling, require custom often admin rights’ installation, depending third-party software. Besides, ’re really practical enforcing FAIR reproducibility guidelines. may noticed (lst-ena-ebi-api?) collected original raw files MD5 peers. Indeed, MD5 file stores 32-character hexadecimal string generated guarantees data integrity (check file’s alterations corruptions, partial downloading, …). mismatch checksums indicates file corrupted. easiest way verify file integrity certainly looping pair original file - MD5 file, using digest package:","code":"# interactive swagger ui https://www.ebi.ac.uk/ena/portal/api/swagger-ui/index.html#/Search%20%26%20Discovery/getResults url_base <- \"https://www.ebi.ac.uk/ena/portal/api\" # all available result types for a given study: ena_avalaible_datasets <- request(url_base) |>   req_url_path_append(\"results\") |>   req_url_query(format=\"tsv\") |>   req_perform() |>   resp_body_string() |>   readr::read_tsv(show_col_types = FALSE)  # of interest for raw files, the `read_run` is certainly the most complete # request for retrieving all potential fields for the read_run dataset # interactive: https://www.ebi.ac.uk/ena/portal/api/swagger-ui/index.html#/Search%20%26%20Discovery/getReturnFields read_run_fields <- request(url_base) |>   req_url_path_append(\"returnFields\") |>   req_url_query(dataPortal=\"ena\", result=\"read_run\", format=\"tsv\") |>   req_perform() |>   resp_body_string() |>   readr::read_tsv(show_col_types = FALSE)  # and finally, for a given project, the direct FTP links to fastq or BAM files # interactive: https://www.ebi.ac.uk/ena/portal/api/swagger-ui/index.html#/Search%20%26%20Discovery/fileReport PRJEB45993_BAM_FASTQ_URLs <- request(url_base) |>   req_url_path_append(\"filereport\") |>   req_url_query(accession=\"PRJEB45993\",                 result=\"read_run\",                 fields=c(\"study_accession\", \"experiment_accession\",\"run_accession\",                          \"fastq_ftp\", \"fastq_md5\",                          \"bam_ftp\",\" bam_md5\"),                 format=\"tsv\",                 download = TRUE,                  .multi = \"comma\") |>   req_perform()  # for whatever reason, piping all these operations lead to time out.  read_run_fields <- read_run_fields |>   resp_body_string() |>   readr::read_tsv(show_col_types = FALSE)  # number of rows: # https://www.ebi.ac.uk/ena/portal/api/filereportcount?result=read_run&accession=PRJEB45993&format=tsv  read_run_rows <- request(url_base) |>   req_url_path_append(\"filereportcount\") |>   req_url_query(accession=\"PRJEB45993\",                 result=\"read_run\",                 format=\"json\") |>   req_perform() |>    resp_body_json() |>    pluck(\"count\") |>    as.integer()  testthat::expect_equal(nrow(read_run_fields),                         read_run_rows) url_base <- \"https://www.ebi.ac.uk/biostudies\" array_identifier <- \"E-MTAB-10519\" ERP_symbol <- request(url_base) |>   req_url_path_append(\"files\") |>   req_url_path_append(array_identifier) |>   req_url_path_append(paste0(array_identifier, \".json\")) |>   req_perform() |>   resp_body_json(simplifyVector = TRUE) |>   pluck(\"section\") |>   pluck(\"links\") |>   pluck(1) |>   dplyr::pull(\"url\") |>   stringr::str_subset(\"^ERP\") library(digest)  # Define the file path and expected MD5 checksum file_path <- \"path/to/your/file.txt\"  # Replace with your file's path expected_md5 <- \"d41d8cd98f00b204e9800998ecf8427e\"  # Replace with the provided checksum  # Compute the MD5 checksum of the file calculated_md5 <- digest(file = file_path, algo = \"md5\", serialize = FALSE)  # testthat::expect_equal(calculated_md5, expected_md5)"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"optionnal-ftp-access-on-biostudies-and-not-ena","dir":"Articles","previous_headings":"Retrieve transcriptomic annotation > The Fly Cell Atlas expression matrix","what":"(Optionnal) FTP access on Biostudies (and not ENA)","title":"Explore and download sequencing databases: hows-to","text":"detailed following code snippet: Biostudies API retrieve comprehensive metadata file given study, latter listing also differences previous ArrayExpress web server. RCurl simplified code list download files along properties (size access rights). Yet, Biostudies repository usually much less comprehensive organised ENA/SRA web server, exhibiting following odd path composition: ftp.sra.ebi.ac.uk/vol1/<file-type>:err|fastq|run/<accession-prefix>/<00-last-digit--full-accession>/<full-accession>/, described thoroughly SRA FTP Structure documentation, preventing downloading whole study without access metadata file localisations.","code":"# 1) not that relevant, too many fields to process, without clear structure url_base <- \"https://www.ebi.ac.uk/biostudies/api/v1/studies\" array_identifier <- \"E-MTAB-10519\" ena_detailled_information <- request(url_base) |>   req_url_path_append(array_identifier) |>   req_perform() |>   resp_body_json()  # 2) easiest process for listing all files in a FTP repo using RCurl base_url <- 'ftp.ebi.ac.uk/biostudies' study_type <- stringr::str_extract(array_identifier, \"^[[[:alpha:]]-]+\") study_suffix <- stringr::str_extract(array_identifier, \"[[:digit:]]{3}$\") # remove the Files/ if you just wish to access general metadata biostudies_ftp_url <- paste(base_url, \"fire\",                             study_type, study_suffix,                             array_identifier, \"Files/\",                             sep = \"/\") # ftp.use.epsv ensures that the request is not rejected by the FTP web server # dirlistonly ensures that we only retrieve filenames from the FTP repository ftp_biostudies <- RCurl::getURL(biostudies_ftp_url,                            ftp.use.epsv = FALSE, dirlistonly = TRUE) |>   stringr::str_split(pattern = \"\\r\\n\", simplify = TRUE) |>   as.vector() |>   str_subset(\"[^[[:blank:]]*]\") # remove empty chains, only composed of white spaces # define the connection port con <-  getCurlHandle(ftp.use.epsv = FALSE) # download all file listed in the FTP repository contents <-  sapply(filenames, getURL, curl = con)"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"optionnal-sra-and-ena-organisational-structure","dir":"Articles","previous_headings":"Retrieve transcriptomic annotation > The Fly Cell Atlas expression matrix","what":"(Optionnal) SRA and ENA organisational structure","title":"Explore and download sequencing databases: hows-to","text":"hierarchical structure SRA (Sequence Read Archive) ENA (European Nucleotide Archive) follows following order granularity: 1.ERP (SRP) represents study project, describing overarching goal sequencing experiment providing metadata research context. ERS (SRS) represents individual biological sample within study. ERX (SRX) represents experiment, describing methodology applied sample, library preparation, sequencing platform sequencing strategy (e.g., RNA-seq, WGS). ERR (SRR) represents specific run, unit raw sequencing data generated experiment. ERR corresponds FASTQ file set FASTQ files containing sequencing reads. Note complex mappings relate objects : Multiple Experiments can grouped together form Study. single Sample can used one Experiment, many--many relationship Samples Experiments. One Experiment can store multiple Runs. Run level one--one relationship Paired-End, assuming FASTQ format used, relationship original files. documentation - Main facilities delivered ENA web server, including programmatic access: Petabyte-scale innovations European Nucleotide Archive, Cochrane et al. (2009). - infographics detailing interconnections prominent gene repositories reported Fig. (fig-sra-structure?). ENA SRA structures layout.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"programmatic-access-to-flybase","dir":"Articles","previous_headings":"","what":"Programmatic access to FlyBase","title":"Explore and download sequencing databases: hows-to","text":"retrieve datasets stored FlyBase, sex, tissue, lab information, automated programmatic manner, can use three approaches, ordered increasing complexity scalability:","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"precomputed-metadata-files","dir":"Articles","previous_headings":"Programmatic access to FlyBase","what":"Precomputed Metadata Files:","title":"Explore and download sequencing databases: hows-to","text":"accessible using FlyBase FTP: - Wiki Documentation Databases structure Overview Download RPKM file: <version> placeholder referring FlyBase release. Alternatively, use: * regex expression standing symbol (case, download two datasets). Extract view file: Analyze R: Alternatively, propose following portable R script replicate wget gunzip functionalities.","code":"wget http://ftp.flybase.org/releases/FB2024_05/precomputed_files/genes/gene_rpkm_report_fb_<version>.tsv.gz wget http://ftp.flybase.org/releases/FB2024_05/precomputed_files/genes/gene_rpkm_*.tsv.gz gunzip gene_rpkm_report_fb_2024_05.tsv.gz RPKM_file <- readr::read_tsv(\"gene_rpkm_report_fb_2024_05.tsv\",                               sep = \"\\t\", header = TRUE) head(RPKM_file) # Define the URL and output file paths url <- \"http://ftp.flybase.org/releases/FB2024_05/precomputed_files/genes/gene_rpkm_report_fb_2024_05.tsv.gz\" output_file <- \"data/RNASeq/gene_rpkm_report_fb_2024_05.tsv.gz\" output_unzipped <- \"data/RNASeq/gene_rpkm_report_fb_2024_05.tsv\"  # Step 1: Download the file response <- GET(url, # `write_disk()` specifies path location of the download file.                 write_disk(output_file, overwrite = TRUE))  # Check if download was successful if (response$status_code == 200) {   cat(\"File downloaded successfully to:\", output_file, \"\\n\") } else {   stop(\"Download failed with status code:\", response$status_code) }  # Step 2: Decompress the file R.utils::gunzip(output_file, output_unzipped, remove = TRUE)"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"using-the-flybase-rest-api-lightweight-approach","dir":"Articles","previous_headings":"Programmatic access to FlyBase","what":"Using the FlyBase REST API lightweight approach:","title":"Explore and download sequencing databases: hows-to","text":"REST API allows dynamic metadata retrieval via programmatic queries. particular, GET request used retrieve data server. however restricted URL length. ’s linux example , using curl command, using one parameter composed several IDs separated commas: Report (sec-modencode-metadata?) R high-user interface, using complementary jsonlite httr2 packages.","code":"curl -X GET \"https://api.flybase.org/api/v1.0/hitlist/fetch/FBlc0000215,FBlc0000216,FBlc0000217\" -H \"Accept: application/json\""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/download.html","id":"using-the-flybase-chado-postgresql-database","dir":"Articles","previous_headings":"Programmatic access to FlyBase","what":"Using the FlyBase Chado PostgreSQL Database","title":"Explore and download sequencing databases: hows-to","text":"’s definitely challenging, also safest scalable approach. brief, import whole database relational scheme local computer server. Chado database wrapper PostgreSQL DBMS, custom functions parse biological samples. Connect FlyBase public instance12. Query metadata datasets: Export results analysis: (nte-chado-basic?), described Direct Chado Query access. However, public read-access, directly exploitable installed SQL client, suitable occasional access, lots users requesting database, can induce lot latency. alternative approach consists first downloading Hosting Chado Database Locally : Step 1: Download PostgreSQL dump files FlyBase FTP site:FlyBase PostgreSQL Dumps. Step 2: Create new PostgreSQL database locally: Step 3: Load downloaded data local database: Step 4: Optimize database13 use: Chado Schema: Learn database schema structure optimize queries Chado Documentation. Fly Atlas 2 database, report section Citing  FlyAtlas 2 & Data Availability / Data Availability. Global SQL databse Original Fly Atlas2 datasets, including RNASeq Microarray -> retrieve information, likely use sql databse rather excel, data aggregated/merged: https://pmc.ncbi.nlm.nih.gov/articles/instance/5753349/bin/gkx976_supp.pdf explanation SQL schema. SnakeMake GitHub repository reproduce results + metadata Fly Atlas 2","code":"psql -h chado.flybase.org -U flybase flybase SELECT * FROM dataset WHERE dataset_id = 'FB2024_05'; \\COPY (SELECT * FROM dataset WHERE dataset_id = 'FB2024_05') TO 'metadata.csv' CSV HEADER; createdb -E UTF-8 my_flybase cat FB*.sql.gz.* | gunzip | psql my_flybase vacuumdb -f -z -v my_flybase"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_bulkRNASeq.html","id":"key-steps-for-bulk-rnaseq-analysis","dir":"Articles","previous_headings":"","what":"Key steps for bulk RNASeq analysis","title":"Analyse bulk RNASeq experiences","text":"Job Savandara Besse, complement .","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"relevance-proof","dir":"Articles","previous_headings":"","what":"Relevance proof","title":"Analyse single cell RNASeq experiences","text":"deconvolutiona glorithm used far infer cellular composition Drosophilia samples Scispace","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"recommended-scrnaseq-pipeline","dir":"Articles","previous_headings":"","what":"Recommended scRNASeq pipeline","title":"Analyse single cell RNASeq experiences","text":"Harmony Seurat deconvExplorer functions retrieve plot marker gene sets","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"the-fca-initiative","dir":"Articles","previous_headings":"","what":"The FCA initiative","title":"Analyse single cell RNASeq experiences","text":"Original paper: Fly Cell Atlas: single-nucleus transcriptomic atlas adult fruit fly, Li et al. (2022). Detailed supplementary material. Two web platforms used automate processing, visualisation downstream analyses Drosophila samples: [SCope]((https://flycellatlas.org/scope) [ASAP]((https://flycellatlas.org/asap) nextflow GitHub repository process 10X Genomic samples. However, relies back-end Nextflow VSP Pipeline, updated anymore, preventing simply pre-processing data raw level re-running scripts.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"import-and-read-loom-and-h5ad-datasets","dir":"Articles","previous_headings":"","what":"Import and read Loom and H5AD Datasets","title":"Analyse single cell RNASeq experiences","text":"Seurat, use - ReadH5AD: Reads .h5ad files (AnnData format) ReadH5Seurat: Imports .h5Seurat files. - ReadLoom: Reads .loom files scRNA-seq data stored Loom format. Alternatively, can use LoomR R package, read_loom_dgCMatrix function (lower processing level), zellkonverter (H5AD), enhanced interoperability Python’s AnnData R’s SingleCellExperiment format (another lightweight alternative Seurat simpler workflows).","code":""},{"path":[]},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"data-import","dir":"Articles","previous_headings":"Standard scRNA-seq Pipeline for Loom/H5AD Datasets","what":"Data Import:","title":"Analyse single cell RNASeq experiences","text":"","code":"library(Seurat) sc_data <- ReadH5AD(\"path/to/dataset.h5ad\") # or for Loom files loom_data <- ReadLoom(\"path/to/dataset.loom\")"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"quality-control-and-normalization","dir":"Articles","previous_headings":"Standard scRNA-seq Pipeline for Loom/H5AD Datasets","what":"Quality Control and Normalization","title":"Analyse single cell RNASeq experiences","text":"Calculate visualize common QC metrics (e.g., percentage mitochondrial genes, number detected genes per cell). Filter poor-quality cells:","code":"sc_data <- subset(sc_data, subset = nFeature_RNA > 200 & nFeature_RNA < 2500 & percent.mt < 5)  sc_data <- NormalizeData(sc_data)"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"feature-selection-and-scaling","dir":"Articles","previous_headings":"Standard scRNA-seq Pipeline for Loom/H5AD Datasets","what":"Feature Selection and scaling","title":"Analyse single cell RNASeq experiences","text":"Identify highly variable features (genes):","code":"sc_data <- FindVariableFeatures(sc_data, selection.method = \"vst\", nfeatures = 2000)  sc_data <- ScaleData(sc_data, vars.to.regress = c(\"percent.mt\"))"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"visualisation-and-quality-control","dir":"Articles","previous_headings":"Standard scRNA-seq Pipeline for Loom/H5AD Datasets","what":"visualisation and quality control","title":"Analyse single cell RNASeq experiences","text":"Perform PCA: Perform UMAP t-SNE visualization: Cluster cells distinct groups:","code":"sc_data <- RunPCA(sc_data, features = VariableFeatures(object = sc_data)) sc_data <- RunUMAP(sc_data, dims = 1:10) sc_data <- FindNeighbors(sc_data, dims = 1:10) sc_data <- FindClusters(sc_data, resolution = 0.5) DimPlot(sc_data, reduction = \"umap\")"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/articles/preprocess_scRNASeq.html","id":"downstream-analyses","dir":"Articles","previous_headings":"Standard scRNA-seq Pipeline for Loom/H5AD Datasets","what":"Downstream analyses","title":"Analyse single cell RNASeq experiences","text":"Differential Expression Analysis Integration Datasets","code":"markers <- FindMarkers(sc_data, ident.1 = 1, ident.2 = 0)"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Bastien CHASSAGNOL. Author, maintainer.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"CHASSAGNOL B (2024). DrosophiliaDeconv: Build bulk single cell RNASeq profiles Drosophila samples. R package version 0.0.0.9000, https://github.com/bastienchassagnol/DrosophiliaDeconv, https://bastienchassagnol.github.io/DrosophiliaDeconv/.","code":"@Manual{,   title = {DrosophiliaDeconv: Build bulk and single cell RNASeq profiles for Drosophila samples},   author = {Bastien CHASSAGNOL},   year = {2024},   note = {R package version 0.0.0.9000, https://github.com/bastienchassagnol/DrosophiliaDeconv},   url = {https://bastienchassagnol.github.io/DrosophiliaDeconv/}, }"},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/index.html","id":"drosophiliadeconv","dir":"","previous_headings":"","what":"Build reference profiles for reference-based deconvolution algorithms for Drosophila samples","title":"Build reference profiles for reference-based deconvolution algorithms for Drosophila samples","text":"GitHub repository adjust existing deconvolution algorithms men mice deconvolve Drosophilia samples.","code":""},{"path":"https://bastienchassagnol.github.io/DrosophiliaDeconv/index.html","id":"useful-links","dir":"","previous_headings":"","what":"Useful links","title":"Build reference profiles for reference-based deconvolution algorithms for Drosophila samples","text":"Cluster IFB account IFB tutorial Single cell tutorials sincellTE school OnDemand localisation Project Savandara Bess omnideconv vignette Improve GSEA marker-based approaches using Cramer-von Mises CDFs","code":""}]
