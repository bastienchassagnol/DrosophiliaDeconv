[
  {
    "objectID": "01_download_datasets_and_metadata.html",
    "href": "01_download_datasets_and_metadata.html",
    "title": "Protocol to downnload and retrieve metadata to build reference datasets",
    "section": "",
    "text": "# Tidyverse libraries\nlibrary(knitr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(stringr) # For regex operations\nlibrary(purrr) # vectorial operations\nlibrary(readr) # read and write tsv and csv files\n\n#  For website exploration\nlibrary(rvest)  # For web scraping and HTML parsing\nlibrary(jsonlite)\nlibrary(httr2) # tailored for HTTP/HTTPS requests\n# both packages are better tailored to FTP protocols\nlibrary(curl) \nlibrary(RCurl)\n\n# For SQL parsing and integration with R\nlibrary(DBI) # open connection with DBMS\nlibrary(dbplyr) # seamless use of dplyr operations with SQL power\nlibrary(RSQLite) # lightweight DBMS\n\n\n# Set global options\nopts_chunk$set(\n  echo = TRUE,         # Show R code in the output\n  eval = FALSE,        # By default, do not execute R code\n  warning = FALSE,     # Suppress warnings in the output\n  message = FALSE,     # Suppress messages in the output\n  fig.width = 7,       # Default figure width\n  fig.height = 5,      # Default figure height\n  fig.align = \"center\" # Centre-align figures\n)\n\n# Set ggplot2 minimal theme for consistency\ntheme_set(theme_minimal())",
    "crumbs": [
      "Download datasets",
      "Tidyverse libraries"
    ]
  },
  {
    "objectID": "01_download_datasets_and_metadata.html#litterature-and-web-scraping",
    "href": "01_download_datasets_and_metadata.html#litterature-and-web-scraping",
    "title": "Protocol to downnload and retrieve metadata to build reference datasets",
    "section": "Litterature and web-scraping",
    "text": "Litterature and web-scraping\n\nRNASeq, tissue-resolved datasets:\n\nthe FlyBase:Drosophila_Online_Resources\nThe Wiki fly page also refers to a a list of tools, Web APIs and curated pipelines to analyse Drosophila samples.\nMost popular and relevant studies are modENCODE tissue and cell lines experiences and FlyAtlas21.\n\n\nscRNASeq datasets:\n\nList of scRNASeq datasets and tools, at the tissue level\nComprehensive list of scRNASeq databases, within a tissue level, using the DRscDB Shiny App\n\nComplete list of Datasets, from FlyAtlas compendium\n\nAll these datasets, with some metadata characterising them, are reported in Excel table data/databases_metadata.xlsx, composed of three sheets:\n\nDatabases references, enumerating the most relevant datasets and RNASeq resources.\nDRscDB Databases Single Cell, listing all scRNASeq datasets having a higher granularity than tissue level (for instance, characterising the cell subtypes composing the digestive tract).\n\n\n(Optionnal) Retrieve programmatically DRscDB single cell databases\n\nToggle the DRscDB Datasets tab.\nRetrieve the metadata dataset using the R web-scraping tool rvest, and select only the most relevant columns:\n\n\nurl_DRscDB &lt;- \"https://www.flyrnai.org/tools/single_cell/web/summary\"\nirrelevant_columns_DRscDB &lt;- c(\"pubmed date\", \"dataset ids\", \"subset names\",\n                               \"species\", \"species id\",\n                               \"reads cell\", \"genes cell\", \"total gene\",\n                               \"batch correction\", \"data in supp table\",\n                               \"web link available\", \"linkout URL\")\nmetadata_DRscDB &lt;- read_html(url_DRscDB) |&gt; # read a HTML file\n  html_table() |&gt; # from the HTML file, select only Tables (on that website, only one found)\n  purrr::pluck(1) |&gt;\n  filter(species %in% \"Drosophila\") |&gt; # Filter on Drosophila species\n  select(-all_of(irrelevant_columns_DRscDB))",
    "crumbs": [
      "Download datasets",
      "Tidyverse libraries"
    ]
  },
  {
    "objectID": "01_download_datasets_and_metadata.html#retrieve-phenotype-annotation-for-each-dataset",
    "href": "01_download_datasets_and_metadata.html#retrieve-phenotype-annotation-for-each-dataset",
    "title": "Protocol to downnload and retrieve metadata to build reference datasets",
    "section": "Retrieve phenotype annotation for each dataset",
    "text": "Retrieve phenotype annotation for each dataset\n\n\n\n\n\n\nMajor issues for retrieving Phenotypes\n\n\n\n\nNo homogeneous database storage (SQL, Excel, SRA, tsv file, …), within and in-between studies\nWhile the FlyBase database is comprehensive, does not store properly annotation data for each tissue, at least for RNASeq samples2.\nNo official and up-to-date BioConductor nor CRAN package to fetch and explore Drosophila databases\n\nConsequence: specific protocol for retrieving such phenotypical annotation for each dataset (I enumerate for the top three popular ones).\n\n\n\nThe ModENCODE pheno data\n\nThe easiest way I could find: retrieve metadata from JBrowse FlyBase Wiki website\n\n\nModENCODE_databases &lt;- read_html(\"https://wiki.flybase.org/wiki/FlyBase:ModENCODE_data_at_FlyBase#RNA-Seq_JBrowse_Track_Listing\") |&gt; \n  html_table() |&gt; # `trim = FALSE` to ensure missing cells are filled correctly \n  purrr::pluck(1) |&gt; \n  filter(`Track section` %in% c(\"Expression&gt;RNA-Seq &gt;modENCODE Transcriptomes  &gt;Tissues\"))\n\n\nModENCODE_databases_formatted &lt;- ModENCODE_databases |&gt; \n  dplyr::filter(stringr::str_detect(`Track section`, \"Tissues$\")) |&gt; \n  select(-`Track section`, - \"JBrowse Track Description\") |&gt; \n  rename(tissue = \"Track name\", FlybaseID=\"FlyBase Dataset(s)\") |&gt; \n  mutate(FlybaseID = lapply(stringr::str_split(FlybaseID,        # split by mE_mRNA, without consuming the separator\n                   pattern = \"(?=mE_mRNA)\"), str_subset, \"[^ ]\")) |&gt;  # remove empty chains \n  tidyr::unchop(FlybaseID) # from compact list of samples per tissue type to tidy format\n\nreadr::write_csv(ModENCODE_databases_formatted, \n                 \"./data/RNASeq/ModENCODE_pheno_data.csv\")\n\n\nAlternative methods:\n\nWhen pasting a HTML table to an Excel Workbook, use Paste with a refreshable web query option\nThe FlyBase RNASeq Search profile, clicking on the tissue and optionally cell line items.\n\n\n\n\n\n\n\n\n\n\n\nScreenshot FlyBase Profile Search\n\n\n\n\n\n\n\n\n\nScreenshot Tissue Samples\n\n\n\n\n\n\n\nScreenshot Cell Line samples\n\n\n\n\n\n\nTo finalise the annotation, we’re going to use the Rest API GET HitList, programmatic access enabled by the FlyBase website:\n\n\n# Which individual experiences we want metadata from?\nflybase_ids &lt;- c(\"FBlc0000215\", \"FBlc0000216\", \"FBlc0000217\")\n\n# Create a URL with IDs delimited by commas\nbase_url &lt;- \"https://api.flybase.org/api/v1.0/hitlist/fetch\"\nmodENCODE_request &lt;- request(base_url) |&gt;\n  # parameter construction defined here: https://flybase.github.io/api/swagger-ui/#/HitList/getFlyBaseHitList\n  req_url_path_append(flybase_ids |&gt; \n                        paste(collapse = \",\")) |&gt; \n  req_perform()\n\n# Check the response status, 200 corresponding to a success\nif (resp_status(modENCODE_request) == 200 & resp_has_body(modENCODE_request)) {\n  # Parse the JSON content into an R object\n  ModENCODE_metadata &lt;-   modENCODE_request |&gt; \n    resp_body_string() |&gt; \n    fromJSON()  |&gt; \n    purrr::pluck(\"resultset\") |&gt; \n    purrr::pluck(\"result\")\n} else {\n  # convert a HTTP error to a R error\n  resp_check_status(modENCODE_request)\n}\n\n\n\nThe FlyAtlas2 pheno data\n\n\n\n\n\n\nExcel workbook and aggregated dataset\n\n\n\nThe Excel Workbook mentioned in the Docs tab section of the official FlyAtlas2 initiative only stores aggregated and averaged information at the tissue level, rendering it useless for proper downstream analyses3.\nIn conclusion, this method and this dataset is irrelevant for most downstream analyses4\n\n\n\nIn the FPKM/RPKM file stored in the web encyclopaedia FlyBase, the column headers identifying uniquely each sample describe the developmental stage, the sex and the original tissue. However, it’s not really practical to parse, nor follows an unique identifier composition. Example: RNA-Seq_Profile_FlyAtlas2_Adult_Female_Brain_(FBlc0003619).\n\nHowever, we can use the same REST API trick mentioned in (API-FlyBase-modENCODE-metadata?) to get some additional information (column title).\n\n\n\nThe most challenging method involves re-running the SQL file provided with the project.\n\nThe name of the SQL file can be retrieved on the FlyAtlas2 website, or, in a somehow twisted way with the following R code5.\n\n\n\nflyatlas_url &lt;- \"https://flyatlas.gla.ac.uk/FlyAtlas2/index.html?page=help\"\nwebpage_text &lt;- read_html(flyatlas_url) |&gt; \n  html_text()\nflyatlas_files &lt;- stringr::str_extract_all(webpage_text, \n                                         \"motif.mvls.gla.ac.uk/downloads/[^\\\\s]+\", simplify = TRUE) |&gt; as.vector()\n\n\nFrom the previous manipulation, two files are available: motif.mvls.gla.ac.uk/downloads/FlyAtlas2_2024.01.08.sql, motif.mvls.gla.ac.uk/downloads/FlyAtlas2_gene_data.xlsx, but only the SQL file is comprehensive, storing gene expressions at the individual level (and not at the aggregated tissue level). i. The SQL relational database is described in details in Supplementary FlyAtlas 2 documents, Table S2, Pages 3-4. Of interest, the Tissue, GeneFPKM and GeneRPM datasets6. ii. Parse the SQL file’s instructions, keeping SQL lines for building Tissue, GeneFPKM and GeneRPM datasets. Usually, building a SQL dataset involves three stages: ensuring the non prior existence of the dataset (if it’s the case, enforce its deletion), define the dataset (name and type of features/columns), and finally, build the dataset itself (indeed, dataset content is hard-coded within the SQL file):\n\n\nsql_instructions &lt;- readr::read_lines('data/FlyAtlas2_2024.01.08.sql',\n                                      skip_empty_rows = TRUE) |&gt;\n  # Keep only lines that are NOT empty nor commented (defined by \"--\")\n  # stringr::str_subset(\"^\\\\s*(--)|/\\\\*\", negate = TRUE) |&gt;\n  stringr::str_subset(\"^\\\\s*(--)|/\\\\*|(UN)?LOCK\", negate = TRUE) |&gt;\n  paste(collapse = \"\\r\\n\") |&gt;\n  str_extract_all(pattern = regex(\"[^;]+;\", multiline = TRUE,dotall = TRUE),\n                  simplify = TRUE) |&gt;\n  as.vector()\n\ntissue_index &lt;- grep(\"CREATE TABLE `Tissue`\", sql_instructions, fixed = TRUE)\ntissue_instructions &lt;- sql_instructions[(tissue_index-1):(tissue_index + 1)]\nreadr::write_lines(tissue_instructions,\n                   \"data/old_tissue_parsing.sql\")\n\n\nUse modern SQL DBMS. Report to section Section 1.2.3 for further details. Major issue with the current Fly Atlas 2 database is their outdated use of the myISAM DBMS standards, for which no official converter exists.\n\n(Optional) Verify your updated SQL instructions can be parsed and executed using your local DBMS manager like mySQL, or even simpler, going on SQL online resources. It turned out that we had to remove an uniqueness constraint on Tissue.Abbreviation feature to run the SQL code.\nExecute the modern SQL file, store transiently the output to a SQLite database, collect and export the results as a standard csv file.\n\n\n# create SQLite database connection\ncon &lt;- dbConnect(RSQLite::SQLite(), \"data/FlyAtlas2_tissue.sqlite\")\n\nsql_statements &lt;- readr::read_lines(\"data/new_tissue.sql\", \n                                    skip_empty_rows = TRUE) |&gt; \n  stringr::str_subset(\"^\\\\s*(--)|/\\\\*|(UN)?LOCK\", negate = TRUE) |&gt;\n  paste(collapse = \"\\r\\n\") |&gt;\n  str_extract_all(pattern = regex(\"[^;]+;\", multiline = TRUE,dotall = TRUE),\n                  simplify = TRUE) |&gt; \n  as.vector()\n\n# Tip: since dbExecute can only perform one query after the other, we use a for-loop to perform sequential SQL queries\nfor (statement in sql_statements) {\n    dbExecute(con, statement)\n}\n\n# testthat::expect_contains(dbListTables(con), \"Tissue\")\n# DBI::dbRemoveTable(con, \"Tissue\")\n\n# Export the tissue dataset to a standard CSV format\ntissue_fly_atlas &lt;- tbl(con, \"Tissue\") |&gt; \n  collect()\nreadr::write_csv(tissue_fly_atlas,\n                 \"data/pheno_data_flyatlas2.csv\")\n# Close the connection once done\ndbDisconnect(con)\n\n\n\n(Optionnal) Convert old SQL version 5 to modern SQL version 9\nWhile functionalities going along the DBI R package would theoretically enable us to execute SQL instructions directly from R, it turns out that the SQL instructions are tailored to MyISAM DBMS software, outdated from 2009, and can not be processed directly by the DBI wrapper. Besides, DBI works best with SQLite DBMS, and the FlyBase SQL file has not been written to that end. To conclude, we have to convert the SQL FlyBase file to modern SQL standards (current version: SQL Server 2022) while ensuring compatibility with SQLite.\nNo dedicated tools, up to my knowledge, have been developed in R to that end. So definitely, the best method, while being intrinsically stochastic, would consist of pairing a LLM using API REST integration with prompt engineering. I propose two code snippets below with a personalised engineered prompt, using chattr, see Listing 1 and tidychatmodels, see Listing 2, respectively7.\n\n\n\n\nListing 1: Use chattr for updating SQL file.\n\n\n# remotes::install_github(\"mlverse/chattr\")\nlibrary(chattr)\n\n# parameter configuration and saving\nchattr_use(\"copilot\")\nchattr_defaults(max_data_files = 10, \n                include_doc_contents = TRUE, \n                max_data_frames = 10,\n                yaml_file = \"vignettes/chattr.yml\")\n\nnew_sql &lt;- chattr(prompt = \"Please convert the SQL file &lt;old-sql-location&gt;, written with SQL version 5, to SQL Server 2022 syntax, while making it compliant with the SQLite DBMS constraints.\",\n       prompt_build = \"Specifically, focus on the following: Change data types to match SQLite standards (e.g., TINYINT to INTEGER, ENUM to TEXT). Replace AUTO_INCREMENT with AUTOINCREMENT. Remove any unsupported SQL syntax in SQLite. Write the resulting modernised SQL file to `data/new_tissue_global.sql`\")\n\n\n\n\n\n\n\n\nListing 2: Use tidychatmodels for updating SQL files.\n\n\n# devtools::install_github(\"AlbertRapp/tidychatmodels\")\nlibrary(tidychatmodels)\n\nchat_openai &lt;- create_chat('openai', Sys.getenv('OAI_DEV_KEY'))|&gt;\n  add_model('gpt-3.5-turbo') |&gt;\n  # add_params(temperature = 0.5, max_tokens = 100) |&gt;\n  # define the general AI agent system\n  add_message(\n    role = 'system',\n    message = 'You are a chatbot that only returns SQL code, whose syntax is\n    compatible with SQL Server 2022 and SQLite DBMS.'\n  ) |&gt;\n  # create your customised prompt\n  add_message(\n    role = 'user',\n    message = 'Convert old sql file &lt;old-sql-path&gt; to recent SQL code,\n    write the output to &lt;new-sql-path&gt;. Specifically: Update data types to align\n    with SQLite standards, such as replacing TINYINT with INTEGER,\n    ENUM with TEXT, and other incompatible types. Change AUTO_INCREMENT to\n    AUTOINCREMENT for primary keys. Remove syntax elements unsupported in SQLite,\n    like full-text indexes, foreign key constraints outside PRAGMA, or stored procedures.\n    Where conflicts between SQL Server and SQLite arise, prioritize SQLite compliance.\n    Output the transformed SQL file as a set of discrete statements, one per line,\n    removing empty or commented lines for clarity. Do not add any empty lines nor comments. Do not add uniqueness constraints. Convert the entire SQL file, without using further rows argument.'\n  )\n\nchat_user_result &lt;- chat_openai |&gt;\n  perform_chat()|&gt;\n  extract_chat(silent = TRUE)\n\n\n\n\nAnd finally, an alternative SQL method, which mixes a bunch of regex expressions and tidyverse manipulations, while guaranteeing a determined output:\n\ntissue_colnames &lt;- grep(\"CREATE TABLE `Tissue`\", sql_instructions,\n                        fixed = TRUE, value = TRUE) |&gt; \n  stringr::str_extract_all(pattern = \"(?&lt;=\\\\`)[[:alnum:]]+(?=\\\\`)\",\n                           simplify = TRUE) |&gt;\n  as.vector() |&gt; \n  unique() |&gt; \n  setdiff(\"Tissue\")\n\ntissue_inputs &lt;- grep(\"INSERT INTO `Tissue`\", sql_instructions,\n                      fixed = TRUE, value = TRUE) |&gt;\n  stringr::str_extract_all(\"(?&lt;=\\\\()([^)]+)(?=\\\\),*)\",\n                                                 simplify = TRUE) |&gt;\n  as.vector() |&gt;\n  stringr::str_replace_all(\"'\", \"\") |&gt;\n  purrr::map(\\(x) stringr::str_split(x, \",\", simplify = TRUE) |&gt;\n               tibble::as_tibble()) |&gt;\n  purrr::list_rbind() |&gt;\n  setNames(nm = tissue_colnames)\n\nreadr::write_csv(tissue_inputs,\n                 file = \"data/pheno_data_flyatlas2.csv\",\n                 escape = \"none\", quote = \"none\")\n\n\n\nThe single cell Fly Cell Atlas pheno data\nOne possibility to retrieve phenotype data would consist of using the BioConductor ArrayExpress package. Unfortunately, it does not seem updated anymore, and was indeed unable to download and parse the two FCA experiences (actually, even the examples from the package documentation can’t be executed anymore)8. Typical pipeline is illustrated below:\n\n#if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n#    install.packages(\"BiocManager\")\n# BiocManager::install(\"ArrayExpress\")\nlibrary(ArrayExpress)\n\n# download both processed and raw datasets\nae_files &lt;- getAE(\"E-MTAB-10628\", type = \"full\", path = \"data/Fly_Cell_Atlas_10x/\")\n\n## Build a an ExpressionSet from the raw data\nae_data_raw &lt;- ae2bioc(mageFiles = ae_files)\n\n## Build a an ExpressionSet from the processed data\nae_data &lt;- procset(ae_files, getcolproc(ae_files)[2])\n\n# equivalent to run ArrayExpress function\nae_data &lt;- ArrayExpress(\"E-MTAB-10628\", path = \"data/Fly_Cell_Atlas_10x/\",\n                        save = TRUE, dataCols = NULL)\n\n\n# use methods from the Biobase::ExpressionSet object to retrieve respectively \npheno_data &lt;- phenoData(ae_data) # phenotype data\nexperiment_data &lt;- experimentData(ae_data) # MIAME experiment annotation (metadata)\ngene_annotation &lt;- fData(ae_data) # gene feature annotation\nexpression_matrix &lt;- exprs(ae_data) # expression matrix data\n\n# worth noted that the ExpressionSet object is well tailored for bulk micro-array \n# and RNASeq, but not really for singlecell RNASeq experiences\n\nAlternatively, you can download both 10X and Smartseq2 experiences and expression data at the count level (raw and after applying TPM normalisation) using the following Single cell Expression Atlas web query, then crossing out both datasets, and finally clicking on the Download 2 entries button feature.\nYou can perform it simultaneously for both datasets, or one dataset after the other, as illustrated in Listing 3 for the https protocol (only a subsetting of files is available through this protocol) and the ftp protocol, Listing 49: \n\n\n\n\nListing 3: Import ArrayExpress datasets using the https protocol\n\n\n# \"https://www.ebi.ac.uk/gxa/sc/experiment/E-MTAB-10628/download/zip?fileType=experiment-metadata&accessKey=\"\n# \"https://www.ebi.ac.uk/gxa/sc/experiment/E-MTAB-10628/download?fileType=experiment-design&accessKey=\"\n#   https://www.ebi.ac.uk/gxa/sc/experiment/E-MTAB-10628/download/zip?fileType=normalised&accessKey=\n#   https://www.ebi.ac.uk/gxa/sc/experiment/E-MTAB-10628/download/zip?fileType=quantification-raw&accessKey=\n\n\n\nbase_url &lt;- \"https://www.ebi.ac.uk/gxa/sc/experiment/\"\narray_identifer &lt;- \"E-MTAB-10628\" # the SmartSeq identifier\n# array_identifer &lt;- \"E-MTAB-10519\" # the 10X identifier\n# we took the example of the metadata zip file, containing idf and sdrf files\n# should be extended to normalised and raw expression files + experiment design\nmetadata_path &lt;- file.path(\"./data/Fly_Cell_Atlas_10x\",\n                           paste0(array_identifer, \"-experiment-metadata\", \".zip\"))\nfca_request &lt;- request(base_url) |&gt;\n  req_url_path_append(array_identifer, \"download\", \"zip\") |&gt; \n  req_url_query(fileType=\"experiment-metadata\", accessKey=\"\") |&gt; \n  req_progress(type = \"down\") |&gt; \n  req_perform(path = metadata_path) \n\nif (resp_has_body(fca_request) & !resp_is_error(fca_request)) {\n  zip::unzip(metadata_path, \n             overwrite = TRUE, exdir = \"./data/Fly_Cell_Atlas_10x\")\n  file.remove(metadata_path)\n}\n\n\n\n\n\n\n\n\nListing 4: Import ArrayExpress datasets using the ftp protocol\n\n\nbase_url &lt;- \"ftp.ebi.ac.uk/pub/databases/microarray/data/atlas/sc_experiments\"\narray_identifier &lt;- \"E-MTAB-10628\"\nftp_url &lt;- paste0(base_url, \"/\", array_identifier, \"/\")\n\n# List all files in the FTP directory\nftp_filenames &lt;- curl_fetch_memory(ftp_url) |&gt;\n  purrr::pluck(\"content\") |&gt;\n  rawToChar() |&gt;\n  # Split by rows (newline character)\n  str_split( pattern = \"\\r\\n\", simplify = TRUE) |&gt;\n  str_subset(\"[^ ]\") |&gt;  # remove empty lines |&gt;\n  # matches one or more characters that are not whitespace, to the end of the string.\n  stringr::str_extract( pattern = \"[^\\\\s]+$\")\n\nidf_index &lt;- grep(pattern = \"idf\", ftp_URLs)\nftp_URLs &lt;- paste0(ftp_url, ftp_filenames)\ndownload.file(ftp_URLs[idf_index],\n              destfile = ftp_filenames[idf_index],\n              method = \"auto\")\n\n\n\n\n\n\n\n\n\n\nArrayExpress datasets\n\n\n\n\n\nArrayExpress datasets are traditionally organized into the following key file types, ordered by increasing level of detail:\n\nInvestigation Description Format (IDF) provides a high-level overview of the experiment.\n\nTitle and Description: Summary of the experiment’s purpose.\nDesign and Protocols: Experimental design and factors studied (e.g., treatments, time points).\nContacts: Information about the authors or submitters.\n\nSample and Data Relationship Format (SDRF): the phenotype data, detailing the mapping between Sample Identifiers, Factor Values and Data File Names\nArray Design Format (ADF) used to provide the design of microarray platforms used in the experiment. Equivalent to featureData, setting up the mapping between the genes sampled and their modern and standard equivalents.\nExpression matrices, as:\n\nRaw Data Files (from FASTQ (for sequencing data) to CEL (for Affymetrix arrays))\nProcessed Data Files (usually counts, after normalization or transformation)",
    "crumbs": [
      "Download datasets",
      "Tidyverse libraries"
    ]
  },
  {
    "objectID": "01_download_datasets_and_metadata.html#retrieve-transcriptomic-annotation",
    "href": "01_download_datasets_and_metadata.html#retrieve-transcriptomic-annotation",
    "title": "Protocol to downnload and retrieve metadata to build reference datasets",
    "section": "Retrieve transcriptomic annotation",
    "text": "Retrieve transcriptomic annotation\n\nThe ModENCODE expression matrix\nThe current RPKM database storing all bulk RNASeq experiences on the comprehensive FlyBase data collection can be retrieved here.\nAlternatively, we can explore the FTP repository, sub-setting the files’ selection to those mentioning gene_rpkm explicitly in their name10 \n\n# FTP URL for the repository\nftp_url &lt;- \"ftp://ftp.flybase.net/releases/current/precomputed_files/genes/\"\n# List all files in the FTP directory\nftp_files &lt;- curl_fetch_memory(ftp_url) |&gt; \n  purrr::pluck(\"content\") |&gt; \n  rawToChar() |&gt; \n  # Split by rows (newline character)\n  str_split( pattern = \"\\r\\n\", simplify = TRUE) |&gt; \n  str_subset(\"[^ ]\") # remove empty lines\n\n# Split each row by columns (tab character) and convert to a data.frame\nftp_files &lt;- as_tibble(do.call(rbind, \n                               str_split(ftp_files, pattern = \"[[:blank:]]{2,}\")))\n\nftp_files &lt;- ftp_files |&gt; \n  tidyr::separate_wider_delim(cols = \"V4\",\n                              delim = \" \", \n                              too_many = c(\"drop\"), # silently remove file renaming\n                              names = c(\"size\", \"month\", \"day\", \"hour\", \"filename\"))\n\nrnaseq_ftp_files &lt;- stringr::str_subset(ftp_files$filename, \n                                         pattern = \"gene_rpkm\")\n\nIn that case, only 2 store gene expression data, namely gene_rpkm_matrix_fb_2024_05.tsv.gz, gene_rpkm_report_fb_2024_05.tsv.gz:\n\ngene_rpkm_matrix_fb_2024_05.tsv.gz: Table description. Note that ModENCODE data is provided with RPKM pre-processed RNA-Seq expression values, while FlyAtlas2 data has been normalised to FPKM units.\ngene_rpkm_report_fb_2024_05.tsv.gz: Table description\n\n\n\nThe FlyAtlas2 expression matrix\n\n\nThe Fly Cell Atlas expression matrix\nDepending on the level of resolution, the new Biostudies website, aiming at replacing the ArrayExpress web server, provides two APIs:\n\nTo download files at the expression level (counts), both normalised and raw counts, we already detailed a programmatic process in Section 1.2.4. Note that in addition to the protocol detailed in previous section, you may alternatively use FTP, curl, Aspera or Globus software. However, these tools usually require to know in advance all file locations and paths to be downloaded. All these tools are further detailed in Biostudies documention.\nThe ENA Portal API (see Listing 5 for programmatic access) can be used to retrieve raw files, at the sequence level (in other words, the bio-informatician job, with possibility to retrieve both FASTQ and BAM files, the latter corresponding to sequences already aligned against a reference genome).\n\n\nIt requires to retrieve the corresponding ENA or Project accession number (usually, the PRJ... is the main identifier, while the ERP... is the second, optional identifier). E-MTAB-10628 is paired with PRJEB45993 project (ENA identifier: ERP130174) and, while E-MTAB-10519 is paired with PRJEB45570 (ENA identifier: ERP129698).\nMore documentation details can be found here, and an interactive swagger UI is reported here.\nWithout this mapping information, you can browse it on the Biostudies website (report to Figure 1 for a snapshot), or programmatically by either parsing the idf file or retrieving the json file associated with the Biostudies online accession (see Listing 6).\n\n\n\n\n\n\n\nFigure 1: ERP and ExpressionSet MAGE mapping\n\n\n\n\n\n\n\nListing 5: The ENA Portal API\n\n\n# interactive swagger ui https://www.ebi.ac.uk/ena/portal/api/swagger-ui/index.html#/Search%20%26%20Discovery/getResults\nurl_base &lt;- \"https://www.ebi.ac.uk/ena/portal/api\"\n# all available result types for a given study:\nena_avalaible_datasets &lt;- request(url_base) |&gt;\n  req_url_path_append(\"results\") |&gt;\n  req_url_query(format=\"tsv\") |&gt;\n  req_perform() |&gt;\n  resp_body_string() |&gt;\n  readr::read_tsv(show_col_types = FALSE)\n\n# of interest for raw files, the `read_run` is certainly the most complete\n# request for retrieving all potential fields for the read_run dataset\n# interactive: https://www.ebi.ac.uk/ena/portal/api/swagger-ui/index.html#/Search%20%26%20Discovery/getReturnFields\nread_run_fields &lt;- request(url_base) |&gt;\n  req_url_path_append(\"returnFields\") |&gt;\n  req_url_query(dataPortal=\"ena\", result=\"read_run\", format=\"tsv\") |&gt;\n  req_perform() |&gt;\n  resp_body_string() |&gt;\n  readr::read_tsv(show_col_types = FALSE)\n\n# and finally, for a given project, the direct FTP links to fastq or BAM files\n# interactive: https://www.ebi.ac.uk/ena/portal/api/swagger-ui/index.html#/Search%20%26%20Discovery/fileReport\nPRJEB45993_BAM_FASTQ_URLs &lt;- request(url_base) |&gt;\n  req_url_path_append(\"filereport\") |&gt;\n  req_url_query(accession=\"PRJEB45993\",\n                result=\"read_run\",\n                fields=c(\"study_accession\", \"experiment_accession\",\"run_accession\",\n                         \"fastq_ftp\", \"fastq_md5\",\n                         \"bam_ftp\",\" bam_md5\"),\n                format=\"tsv\",\n                download = TRUE, \n                .multi = \"comma\") |&gt;\n  req_perform() \n# for whatever reason, piping all these operations lead to time out. \nread_run_fields &lt;- read_run_fields |&gt;\n  resp_body_string() |&gt;\n  readr::read_tsv(show_col_types = FALSE)\n\n# number of rows:\n# https://www.ebi.ac.uk/ena/portal/api/filereportcount?result=read_run&accession=PRJEB45993&format=tsv\n\nread_run_rows &lt;- request(url_base) |&gt;\n  req_url_path_append(\"filereportcount\") |&gt;\n  req_url_query(accession=\"PRJEB45993\",\n                result=\"read_run\",\n                format=\"json\") |&gt;\n  req_perform() |&gt; \n  resp_body_json() |&gt; \n  pluck(\"count\") |&gt; \n  as.integer()\n\ntestthat::expect_equal(nrow(read_run_fields), \n                       read_run_rows)\n\n\n\n\n\n\n\n\nListing 6: The ENA Portal API\n\n\nurl_base &lt;- \"https://www.ebi.ac.uk/biostudies\"\narray_identifier &lt;- \"E-MTAB-10519\"\nERP_symbol &lt;- request(url_base) |&gt;\n  req_url_path_append(\"files\") |&gt;\n  req_url_path_append(array_identifier) |&gt;\n  req_url_path_append(paste0(array_identifier, \".json\")) |&gt;\n  req_perform() |&gt;\n  resp_body_json(simplifyVector = TRUE) |&gt;\n  pluck(\"section\") |&gt;\n  pluck(\"links\") |&gt;\n  pluck(1) |&gt;\n  dplyr::pull(\"url\") |&gt;\n  stringr::str_subset(\"^ERP\")\n\n\n\n\nOf note, without REST API programmatic access, the BioConductor SRAdb package provides programmatic access to metadata from the Sequence Read Archive (SRA) and ENA (European Nucleotide Archive) from R11.\nHowever, it relies on a strong SQLite dependency, has not been maintained for more than two years, and requires prior installation of the SQL metadata database, implying strong storage and memory-resource. Besides, search of the database has been streamlined by the development of the ENA Browser API.\nOther methods for non-programmers are reported in Download Files ENA Documentation, with popular tools like ENA Browser, ENA FTP Downloader GUI tool, Globus or Aspera. While practical for downloading thousands of files, with dedicated facilities to manage parallel downloading or error handling, they require custom and often admin rights’ installation, depending on third-party software. Besides, they’re not really practical for enforcing FAIR and reproducibility guidelines.\n\n\n\n\n\n\nMD5 role\n\n\n\n\n\nYou may have noticed in Listing 5 that we have collected both original raw files and their MD5 peers. Indeed, the MD5 file stores a 32-character hexadecimal string generated that guarantees data integrity (check for file’s alterations or corruptions, partial downloading, …). Any mismatch in checksums indicates that the file has been corrupted. The easiest way to verify file integrity is certainly by looping over each pair of original file - MD5 file, using the digest package:\nlibrary(digest)\n\n# Define the file path and expected MD5 checksum\nfile_path &lt;- \"path/to/your/file.txt\"  # Replace with your file's path\nexpected_md5 &lt;- \"d41d8cd98f00b204e9800998ecf8427e\"  # Replace with the provided checksum\n\n# Compute the MD5 checksum of the file\ncalculated_md5 &lt;- digest(file = file_path, algo = \"md5\", serialize = FALSE)\n\n# testthat::expect_equal(calculated_md5, expected_md5)\n\n\n\n\n(Optionnal) FTP access on Biostudies (and not ENA)\nWe detailed in the following code snippet:\n\nthe Biostudies API to retrieve the most comprehensive metadata file for a given study, the latter listing also differences with the previous ArrayExpress web server.\nA RCurl simplified code to list and download all files along with their properties (such as size or access rights). Yet, the Biostudies repository is usually much less comprehensive and organised as the ENA/SRA web server, while exhibiting the following odd path composition: ftp.sra.ebi.ac.uk/vol1/&lt;file-type&gt;:err|fastq|run/&lt;accession-prefix&gt;/&lt;00-last-digit-of-full-accession&gt;/&lt;full-accession&gt;/, described thoroughly in SRA FTP Structure documentation, and preventing downloading the whole study without access to metadata and file localisations.\n\n\n# 1) not that relevant, too many fields to process, without clear structure\nurl_base &lt;- \"https://www.ebi.ac.uk/biostudies/api/v1/studies\"\narray_identifier &lt;- \"E-MTAB-10519\"\nena_detailled_information &lt;- request(url_base) |&gt;\n  req_url_path_append(array_identifier) |&gt;\n  req_perform() |&gt;\n  resp_body_json()\n\n# 2) easiest process for listing all files in a FTP repo using RCurl\nbase_url &lt;- 'ftp.ebi.ac.uk/biostudies'\nstudy_type &lt;- stringr::str_extract(array_identifier, \"^[[[:alpha:]]-]+\")\nstudy_suffix &lt;- stringr::str_extract(array_identifier, \"[[:digit:]]{3}$\")\n# remove the Files/ if you just wish to access general metadata\nbiostudies_ftp_url &lt;- paste(base_url, \"fire\",\n                            study_type, study_suffix,\n                            array_identifier, \"Files/\",\n                            sep = \"/\")\n# ftp.use.epsv ensures that the request is not rejected by the FTP web server\n# dirlistonly ensures that we only retrieve filenames from the FTP repository\nftp_biostudies &lt;- RCurl::getURL(biostudies_ftp_url,\n                           ftp.use.epsv = FALSE, dirlistonly = TRUE) |&gt;\n  stringr::str_split(pattern = \"\\r\\n\", simplify = TRUE) |&gt;\n  as.vector() |&gt;\n  str_subset(\"[^[[:blank:]]*]\") # remove empty chains, only composed of white spaces\n# define the connection port\ncon &lt;-  getCurlHandle(ftp.use.epsv = FALSE)\n# download all file listed in the FTP repository\ncontents &lt;-  sapply(filenames, getURL, curl = con)\n\n\n\n(Optionnal) SRA and ENA organisational structure\n\n\n\n\n\n\nENA hierarchical structure\n\n\n\nThe hierarchical structure in the SRA (Sequence Read Archive) or ENA (European Nucleotide Archive) follows the following order of granularity:\n1.ERP (or SRP) represents a study or project, describing the overarching goal of the sequencing experiment and providing metadata about the research context.\n\nERS (or SRS) represents an individual biological sample within a study.\nERX (or SRX) represents an experiment, describing the methodology applied to a sample, such as library preparation, sequencing platform and sequencing strategy (e.g., RNA-seq, WGS).\nThe ERR (or SRR) represents a specific run, which is the unit of raw sequencing data generated from an experiment. Each ERR corresponds to a FASTQ file or set of FASTQ files containing sequencing reads.\n\nNote that complex mappings relate these objects to each other:\n\nMultiple Experiments can be grouped together to form a Study.\nA single Sample can be used by more than one Experiment, so there is a many-to-many relationship between Samples and Experiments.\nOne Experiment can store multiple Runs.\nOnly the Run level has a one-to-one relationship or Paired-End, assuming the FASTQ format was used, relationship with the original files.\n\n\n\nMore documentation on the - Main facilities delivered by the ENA web server, including programmatic access: Petabyte-scale innovations at the European Nucleotide Archive, from (cochrane2009nar?). - Some infographics for detailing the interconnections between the most prominent gene repositories are reported in Fig. Figure 2.\n\n\n\n\n\n\n\n\nGene Expression Repositories Explained\n\n\n\n\n\n\n\nPipeline to pair raw data from ENA/SRA, metadata from Biosamples and reference genoms from Ensembl and RefSeq\n\n\n\n\n\n\n\n\n\nSRA structure\n\n\n\n\n\n\n\nThe relational data model for ENA reads: a study is associated with samples that are themselves related to runs. Besides, each run is associated with an experiment identifier describing which technologu has been used to perform it. Underlying this data model is an API that provides abstraction from the nature of the data file system, returning read data upon request based on read identifiers.\n\n\n\n\n\n\nFigure 2: ENA and SRA structures and layout.",
    "crumbs": [
      "Download datasets",
      "Tidyverse libraries"
    ]
  },
  {
    "objectID": "01_download_datasets_and_metadata.html#precomputed-metadata-files",
    "href": "01_download_datasets_and_metadata.html#precomputed-metadata-files",
    "title": "Protocol to downnload and retrieve metadata to build reference datasets",
    "section": "Precomputed Metadata Files:",
    "text": "Precomputed Metadata Files:\nThey are accessible using the FlyBase FTP: - Wiki Documentation of the Databases structure and Overview\n\n\n\n\n\n\nOnly for Linux users: wget access\n\n\n\n\n\n\nDownload the RPKM file:\n\nwget http://ftp.flybase.org/releases/FB2024_05/precomputed_files/genes/gene_rpkm_report_fb_&lt;version&gt;.tsv.gz \nwhere &lt;version&gt; is a placeholder referring to a FlyBase release. Alternatively, use:\nwget http://ftp.flybase.org/releases/FB2024_05/precomputed_files/genes/gene_rpkm_*.tsv.gz \nwhere * is a regex expression standing for any symbol (in that case, it should download two datasets).\n\nExtract and view the file:\n\ngunzip gene_rpkm_report_fb_2024_05.tsv.gz\n\nAnalyze in R:\n\nRPKM_file &lt;- readr::read_tsv(\"gene_rpkm_report_fb_2024_05.tsv\", \n                             sep = \"\\t\", header = TRUE)\nhead(RPKM_file)\n\n\n\nAlternatively, we propose the following portable R script to replicate the wget and gunzip functionalities.\n\n# Define the URL and output file paths\nurl &lt;- \"http://ftp.flybase.org/releases/FB2024_05/precomputed_files/genes/gene_rpkm_report_fb_2024_05.tsv.gz\"\noutput_file &lt;- \"data/RNASeq/gene_rpkm_report_fb_2024_05.tsv.gz\"\noutput_unzipped &lt;- \"data/RNASeq/gene_rpkm_report_fb_2024_05.tsv\"\n\n# Step 1: Download the file\nresponse &lt;- GET(url, # `write_disk()` specifies path location of the download file.\n                write_disk(output_file, overwrite = TRUE))\n\n# Check if download was successful\nif (response$status_code == 200) {\n  cat(\"File downloaded successfully to:\", output_file, \"\\n\")\n} else {\n  stop(\"Download failed with status code:\", response$status_code)\n}\n\n# Step 2: Decompress the file\nR.utils::gunzip(output_file, output_unzipped, remove = TRUE)",
    "crumbs": [
      "Download datasets",
      "Tidyverse libraries"
    ]
  },
  {
    "objectID": "01_download_datasets_and_metadata.html#using-the-flybase-rest-api-lightweight-approach",
    "href": "01_download_datasets_and_metadata.html#using-the-flybase-rest-api-lightweight-approach",
    "title": "Protocol to downnload and retrieve metadata to build reference datasets",
    "section": "Using the FlyBase REST API lightweight approach:",
    "text": "Using the FlyBase REST API lightweight approach:\nThe REST API allows for dynamic metadata retrieval via programmatic queries. In particular, the GET request is used to retrieve data from a server. It is however restricted by URL length. Here’s a linux example below, using the curl command, and using one parameter composed of several IDs separated by commas:\ncurl -X GET \"https://api.flybase.org/api/v1.0/hitlist/fetch/FBlc0000215,FBlc0000216,FBlc0000217\" -H \"Accept: application/json\"\nReport to (modencode-metadata?) for a R high-user interface, using the complementary between the jsonlite and httr2 packages.",
    "crumbs": [
      "Download datasets",
      "Tidyverse libraries"
    ]
  },
  {
    "objectID": "01_download_datasets_and_metadata.html#using-the-flybase-chado-postgresql-database",
    "href": "01_download_datasets_and_metadata.html#using-the-flybase-chado-postgresql-database",
    "title": "Protocol to downnload and retrieve metadata to build reference datasets",
    "section": "Using the FlyBase Chado PostgreSQL Database",
    "text": "Using the FlyBase Chado PostgreSQL Database\nThat’s definitely the most challenging, but also the safest and most scalable approach. In brief, you import the whole database and its relational scheme to your local computer or server.\nThe Chado database is a wrapper of the PostgreSQL DBMS, with custom functions to parse biological samples.\n\n\n\n\n\n\nNote 1: Use the Chado Database\n\n\n\n\n\n\nConnect to the FlyBase public instance12.\n\n\npsql -h chado.flybase.org -U flybase flybase\n\n\nQuery metadata for datasets:\n\n\nSELECT *\nFROM dataset\nWHERE dataset_id = 'FB2024_05';\n\n\nExport results for further analysis:\n\n\n\\COPY (SELECT * FROM dataset WHERE dataset_id = 'FB2024_05') TO 'metadata.csv' CSV HEADER;\n\n\n\n\n\nIn Note 1, we described the Direct Chado Query access. However, this public and read-only access, directly exploitable with an installed SQL client, is suitable only for occasional access, and with lots of users requesting the database, can induce a lot of latency.\nThe alternative approach consists of first downloading and Hosting the Chado Database Locally :\n\nStep 1: Download the PostgreSQL dump files from the FlyBase FTP site:\nFlyBase PostgreSQL Dumps.\nStep 2: Create a new PostgreSQL database locally:\n\ncreatedb -E UTF-8 my_flybase\n\nStep 3: Load the downloaded data into your local database:\n\ncat FB*.sql.gz.* | gunzip | psql my_flybase\n\nStep 4: Optimize the database13 for use:\n\nvacuumdb -f -z -v my_flybase\n\nChado Schema: Learn the database schema structure to optimize queries with the Chado Documentation.\n\n\n\n\n\n\n\n\n\n\n\nMethod\nAdvantages\nDisadvantages\n\n\n\n\nPrecomputed Metadata\nEasy to use; requires minimal setup\nLimited to static metadata releases\n\n\nREST API\nDynamic queries; programmatic access\nRequires scripting\n\n\nChado PostgreSQL\nComprehensive and direct database access\nHigh setup complexity; requires SQL\n\n\n\n\n\nFly Atlas 2 database, then report to section Citing  FlyAtlas 2 & Data Availability / Data Availability.\nGlobal SQL databse\nOriginal Fly Atlas2 datasets, including both RNASeq and Microarray -&gt; to retrieve all information, likely to use the sql databse rather than excel, where data is aggregated/merged: https://pmc.ncbi.nlm.nih.gov/articles/instance/5753349/bin/gkx976_supp.pdf for an explanation of the SQL schema.\nSnakeMake GitHub repository to reproduce all the results + metadata for Fly Atlas 2",
    "crumbs": [
      "Download datasets",
      "Tidyverse libraries"
    ]
  },
  {
    "objectID": "01_download_datasets_and_metadata.html#footnotes",
    "href": "01_download_datasets_and_metadata.html#footnotes",
    "title": "Protocol to downnload and retrieve metadata to build reference datasets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFlyAtlas was a compendium of micro-array datasets, and has been outdated by the release of FlyAtlas2↩︎\nIt appears that the Large Dataset metadata file only contains primary and foreign keys for each Drosophila experience and individual, constraining to an external linkout to retrieve such metadata↩︎\nIndeed, most reference-based deconvolution algorithms require an initial stringent feature selection process, requiring both estimation of the mean and individual gene variances.↩︎\nBesides, as shown in (Zeeberg et al. 2004) demonstrated that up to \\(30\\%\\) published papers contain mangled gene names, most of them being induced by Excel spreadsheets’ auto-completion.↩︎\nUnfortunately, we can’t directly parse the http(s) repository as proposed in section Section 1.3.1. Indeed, most of the recent HTTPs data repositories prevent from navigating their file structures without proper authentication↩︎\nOf note, both are also available at the transcript level↩︎\nReturns inconsistent or uncomplete results unfortunately. Had to test the local ollamar package.**) or the recent elmer package↩︎\nPart of the explanation lies in migration from ArrayExpress to BioStudies API standards, as reported in Programmatic access ArrayExpress↩︎\nAll the files that can be downloaded from a single cell experience are detailed in EBI Download Help tabset↩︎\nOnly RPKM and FPKM pre-processing are available on FlyBase.↩︎\nVignette for the package and Recent project use case↩︎\nIn details, the following credentials are expected: Hostname:(chado.flybase.org), Database:(flybase), User:(flybase), no password and Port: 5432↩︎\nThis approach is recommended for high-performance needs.↩︎",
    "crumbs": [
      "Download datasets",
      "Tidyverse libraries"
    ]
  },
  {
    "objectID": "02_02_preprocess_scRNASeq.html",
    "href": "02_02_preprocess_scRNASeq.html",
    "title": "Relevance proof",
    "section": "",
    "text": "No deconvolutiona glorithm used so far to infer the cellular composition from Drosophilia samples\nWith Scispace",
    "crumbs": [
      "Home",
      "Generate reference profiles",
      "Relevance proof"
    ]
  },
  {
    "objectID": "02_02_preprocess_scRNASeq.html#the-fca-initiative",
    "href": "02_02_preprocess_scRNASeq.html#the-fca-initiative",
    "title": "Relevance proof",
    "section": "The FCA initiative",
    "text": "The FCA initiative\n\nOriginal paper: Fly Cell Atlas: a single-nucleus transcriptomic atlas of the adult fruit fly, from (li2022s?).\nDetailed supplementary material.\n\n Two web platforms have been used to automate the processing, visualisation and downstream analyses of Drosophila samples:\n\n[SCope]((https://flycellatlas.org/scope)\n[ASAP]((https://flycellatlas.org/asap)\n\n\n\n\nKey steps of the FlyCellAtlas pipeline\n\n\n\nThe nextflow GitHub repository to process the 10X Genomic samples. However, it relies in the back-end on the Nextflow VSP Pipeline, which is not updated anymore, preventing from simply pre-processing data at the raw level by re-running the scripts.",
    "crumbs": [
      "Home",
      "Generate reference profiles",
      "Relevance proof"
    ]
  },
  {
    "objectID": "02_02_preprocess_scRNASeq.html#import-and-read-loom-and-h5ad-datasets",
    "href": "02_02_preprocess_scRNASeq.html#import-and-read-loom-and-h5ad-datasets",
    "title": "Relevance proof",
    "section": "Import and read Loom and H5AD Datasets",
    "text": "Import and read Loom and H5AD Datasets\nWith Seurat, use - ReadH5AD: Reads .h5ad files (AnnData format) or ReadH5Seurat: Imports .h5Seurat files. - ReadLoom: Reads .loom files for scRNA-seq data stored in the Loom format.\nAlternatively, you can use LoomR R package, with the read_loom_dgCMatrix function (lower processing level), and zellkonverter (for H5AD), with enhanced interoperability between Python’s AnnData and R’s SingleCellExperiment format (another lightweight alternative to Seurat for simpler workflows).",
    "crumbs": [
      "Home",
      "Generate reference profiles",
      "Relevance proof"
    ]
  },
  {
    "objectID": "02_02_preprocess_scRNASeq.html#standard-scrna-seq-pipeline-for-loomh5ad-datasets",
    "href": "02_02_preprocess_scRNASeq.html#standard-scrna-seq-pipeline-for-loomh5ad-datasets",
    "title": "Relevance proof",
    "section": "Standard scRNA-seq Pipeline for Loom/H5AD Datasets",
    "text": "Standard scRNA-seq Pipeline for Loom/H5AD Datasets\n\nData Import:\nlibrary(Seurat)\nsc_data &lt;- ReadH5AD(\"path/to/dataset.h5ad\")\n# or for Loom files\nloom_data &lt;- ReadLoom(\"path/to/dataset.loom\")\n\n\nQuality Control and Normalization\n\nCalculate and visualize common QC metrics (e.g., percentage of mitochondrial genes, number of detected genes per cell).\nFilter out poor-quality cells:\n\nsc_data &lt;- subset(sc_data, subset = nFeature_RNA &gt; 200 & nFeature_RNA &lt; 2500 & percent.mt &lt; 5)\n sc_data &lt;- NormalizeData(sc_data)\n\n\nFeature Selection and scaling\n\nIdentify highly variable features (genes):\n\nsc_data &lt;- FindVariableFeatures(sc_data, selection.method = \"vst\", nfeatures = 2000)\n sc_data &lt;- ScaleData(sc_data, vars.to.regress = c(\"percent.mt\"))\n\n\nvisualisation and quality control\n\nPerform PCA:\n\nsc_data &lt;- RunPCA(sc_data, features = VariableFeatures(object = sc_data))\n\nPerform UMAP or t-SNE for visualization:\n\nsc_data &lt;- RunUMAP(sc_data, dims = 1:10)\n\nCluster the cells into distinct groups:\n\nsc_data &lt;- FindNeighbors(sc_data, dims = 1:10)\nsc_data &lt;- FindClusters(sc_data, resolution = 0.5)\nDimPlot(sc_data, reduction = \"umap\")\n\n\nDownstream analyses\n\nDifferential Expression Analysis\n\nmarkers &lt;- FindMarkers(sc_data, ident.1 = 1, ident.2 = 0)\n\nIntegration with Other Datasets",
    "crumbs": [
      "Home",
      "Generate reference profiles",
      "Relevance proof"
    ]
  }
]